{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPHN7PJgKOzb"
   },
   "source": [
    "# Interacting with CLIP\n",
    "\n",
    "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1hkDT38hSaP",
    "outputId": "70a44964-883d-4fd0-b95a-2c7f2b19aca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLFS29hnhlY4",
    "outputId": "11779e1e-8bdd-4167-c18e-d26bdd6b67db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBRVTY9lbGm8",
    "outputId": "f06fd2fd-6126-475b-87d0-b10aa3b7da49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 102,007,137\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "# model, preprocess = clip.load(\"ViT-B/32\")\n",
    "# model, preprocess = clip.load(\"ViT-L/14\")\n",
    "# model, preprocess = clip.load(\"RN50x64\")\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21slhZGCqANb"
   },
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
    "\n",
    "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6cpiIFHp9N6",
    "outputId": "880cb98e-1e5e-430e-8b59-4bf35fa554f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7fd3154d0ee0>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Replace the original image pre-processor as we don't want to normalize the image using other dataset stats\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        ToTensor(),\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        # CenterCrop(n_px),\n",
    "        # _convert_image_to_rgb,\n",
    "        # ToTensor(),\n",
    "        # Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) #For CIFAR-10 dataset\n",
    "    ])\n",
    "\n",
    "\n",
    "# preprocess = _transform(model.input_resolution.item())\n",
    "preprocess = _transform(model.visual.input_resolution)\n",
    "\n",
    "print(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./CIFAR-dataset', train=True, download=False,\n",
    "                            # transform=transform_train)\n",
    "                            transform=preprocess)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                        #   batch_size=args.batch_size,\n",
    "                                          batch_size=1000,\n",
    "                                          shuffle=False, num_workers=8)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./CIFAR-dataset', train=False, download=False,\n",
    "                        #    transform=transform_test)\n",
    "                           transform=preprocess)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000,\n",
    "                                         shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    image_features = []\n",
    "    for idx, (image, label) in enumerate(trainloader):\n",
    "        image = image.cuda()\n",
    "        feature = model.encode_image(image)\n",
    "        image_features.append(feature)\n",
    "\n",
    "    image_features = torch.vstack(image_features)\n",
    "    torch.save(image_features, f\"CIFAR-dataset/train_clip_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    image_features = []\n",
    "    for idx, (image, label) in enumerate(testloader):\n",
    "        image = image.cuda()\n",
    "        feature = model.encode_image(image)\n",
    "        image_features.append(feature)\n",
    "\n",
    "    image_features = torch.vstack(image_features)\n",
    "    torch.save(image_features, f\"CIFAR-dataset/test_clip_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.load(f\"CIFAR-dataset/train_clip_features.pt\")\n",
    "test_features = torch.load(f\"CIFAR-dataset/test_clip_features.pt\")\n",
    "\n",
    "train_labels = np.array(trainset.targets)\n",
    "test_labels = np.array(testset.targets)\n",
    "\n",
    "label_map = {'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByLabel(label, data, target):\n",
    "    label_id = label_map[label]\n",
    "    label_idx = np.where(target == label_id)\n",
    "    filtered_data = data[label_idx]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(texts, image_features):\n",
    "\n",
    "    text_tokens = clip.tokenize(texts).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text = ['thigh']\n",
      "Automobile: min = 0.06357361376285553, max = 0.18334342539310455, mean = 0.13728711009025574\n",
      "Cat: min = 0.07966386526823044, max = 0.19267237186431885, mean = 0.15150730311870575\n"
     ]
    }
   ],
   "source": [
    "automobile_tf = filterByLabel('automobile', train_features, train_labels)\n",
    "cat_tf = filterByLabel('cat', train_features, train_labels)\n",
    "dog_tf = filterByLabel('dog', train_features, train_labels)\n",
    "\n",
    "texts = [\"thigh\"] #tail, wheel, fur, eyes\n",
    "\n",
    "automobile_sim = getSimilarity(texts, automobile_tf)\n",
    "cat_sim = getSimilarity(texts, cat_tf)\n",
    "\n",
    "print(f\"Text = {texts}\")\n",
    "print(f\"Automobile: min = {automobile_sim.min()}, max = {automobile_sim.max()}, mean = {automobile_sim.mean()}\")\n",
    "print(f\"Cat: min = {cat_sim.min()}, max = {cat_sim.max()}, mean = {cat_sim.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text = ['eyes']\n",
      "Automobile: min = 0.08654673397541046, max = 0.18425166606903076, mean = 0.1405373513698578\n",
      "Cat: min = 0.07684467732906342, max = 0.207831472158432, mean = 0.15472307801246643\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text = {texts}\")\n",
    "print(f\"Automobile: min = {automobile_sim.min()}, max = {automobile_sim.max()}, mean = {automobile_sim.mean()}\")\n",
    "print(f\"Cat: min = {cat_sim.min()}, max = {cat_sim.max()}, mean = {cat_sim.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text = windsheild\n",
      "Automobile: min = 0.12548942863941193, max = 0.24190689623355865, mean = 0.18964295089244843\n",
      "Cat: min = 0.10408192127943039, max = 0.22738224267959595, mean = 0.1690063774585724\n",
      "\n",
      "\n",
      "Text = wheel\n",
      "Automobile: min = 0.10472864657640457, max = 0.23049549758434296, mean = 0.17556533217430115\n",
      "Cat: min = 0.08182226121425629, max = 0.2035258263349533, mean = 0.1606726348400116\n",
      "\n",
      "\n",
      "Text = headlight\n",
      "Automobile: min = 0.09691011905670166, max = 0.2295389026403427, mean = 0.18348842859268188\n",
      "Cat: min = 0.07878608256578445, max = 0.21632120013237, mean = 0.1671178638935089\n",
      "\n",
      "\n",
      "Text = taillight\n",
      "Automobile: min = 0.0675494521856308, max = 0.2327323704957962, mean = 0.1751442700624466\n",
      "Cat: min = 0.047242388129234314, max = 0.2042112946510315, mean = 0.15766076743602753\n",
      "\n",
      "\n",
      "Text = door\n",
      "Automobile: min = 0.09759103506803513, max = 0.20653468370437622, mean = 0.1551189422607422\n",
      "Cat: min = 0.1016419380903244, max = 0.21448925137519836, mean = 0.15745913982391357\n",
      "\n",
      "\n",
      "Text = tail\n",
      "Automobile: min = 0.06236064061522484, max = 0.18304242193698883, mean = 0.13703596591949463\n",
      "Cat: min = 0.04518873244524002, max = 0.20295007526874542, mean = 0.15598955750465393\n",
      "\n",
      "\n",
      "Text = fur\n",
      "Automobile: min = 0.0627601146697998, max = 0.1770177036523819, mean = 0.12189074605703354\n",
      "Cat: min = 0.08846200257539749, max = 0.1998376101255417, mean = 0.14412212371826172\n",
      "\n",
      "\n",
      "Text = eyes\n",
      "Automobile: min = 0.08654673397541046, max = 0.18425166606903076, mean = 0.1405387818813324\n",
      "Cat: min = 0.07684467732906342, max = 0.207831472158432, mean = 0.15472492575645447\n",
      "\n",
      "\n",
      "Text = nose\n",
      "Automobile: min = 0.09277111291885376, max = 0.21053412556648254, mean = 0.15834836661815643\n",
      "Cat: min = 0.10403107106685638, max = 0.21943992376327515, mean = 0.1683548390865326\n",
      "\n",
      "\n",
      "Text = mouth\n",
      "Automobile: min = 0.07834732532501221, max = 0.21509981155395508, mean = 0.14782577753067017\n",
      "Cat: min = 0.09552758187055588, max = 0.22498981654644012, mean = 0.16394168138504028\n",
      "\n",
      "\n",
      "Text = face\n",
      "Automobile: min = 0.08917506039142609, max = 0.20332331955432892, mean = 0.14948344230651855\n",
      "Cat: min = 0.08888137340545654, max = 0.2313423752784729, mean = 0.16246318817138672\n",
      "\n",
      "\n",
      "Text = claw\n",
      "Automobile: min = 0.08081067353487015, max = 0.1940688192844391, mean = 0.14582417905330658\n",
      "Cat: min = 0.08141801506280899, max = 0.19861292839050293, mean = 0.15832562744617462\n",
      "\n",
      "\n",
      "Text = teeth\n",
      "Automobile: min = 0.06946483254432678, max = 0.19965292513370514, mean = 0.13586556911468506\n",
      "Cat: min = 0.09539894014596939, max = 0.21686668694019318, mean = 0.15348051488399506\n",
      "\n",
      "\n",
      "Text = wiskers\n",
      "Automobile: min = 0.08876287192106247, max = 0.18157441914081573, mean = 0.1451491117477417\n",
      "Cat: min = 0.08388841897249222, max = 0.2223542034626007, mean = 0.1680469661951065\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_list = [\"windsheild\", \"wheel\", \"headlight\", \"taillight\", \"door\", \"tail\", \"fur\", \"eyes\", \"nose\", \"ear\", \"mouth\", \"face\", \"claw\", \"teeth\", \"wiskers\", \"cheekbone\", \"chest\", \"thigh\"]\n",
    "\n",
    "for texts in texts_list:\n",
    "\n",
    "    automobile_sim = getSimilarity(texts, automobile_tf)\n",
    "    cat_sim = getSimilarity(texts, cat_tf)\n",
    "\n",
    "    print(f\"Text = {texts}\")\n",
    "    print(f\"Automobile: min = {automobile_sim.min()}, max = {automobile_sim.max()}, mean = {automobile_sim.mean()}\")\n",
    "    print(f\"Cat: min = {cat_sim.min()}, max = {cat_sim.max()}, mean = {cat_sim.mean()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text = bark\n",
      "Dog: min = 0.09063378721475601, max = 0.22089596092700958, mean = 0.16723042726516724\n",
      "Cat: min = 0.09284797310829163, max = 0.21647988259792328, mean = 0.16228678822517395\n",
      "\n",
      "\n",
      "Text = meow\n",
      "Dog: min = 0.08629445731639862, max = 0.1992775797843933, mean = 0.159098282456398\n",
      "Cat: min = 0.08407150954008102, max = 0.21803516149520874, mean = 0.16171546280384064\n",
      "\n",
      "\n",
      "Text = tail\n",
      "Dog: min = 0.07553678005933762, max = 0.2036104053258896, mean = 0.15588702261447906\n",
      "Cat: min = 0.04518873244524002, max = 0.20295007526874542, mean = 0.15598955750465393\n",
      "\n",
      "\n",
      "Text = fur\n",
      "Dog: min = 0.07937782257795334, max = 0.19462312757968903, mean = 0.14418521523475647\n",
      "Cat: min = 0.08846200257539749, max = 0.1998376101255417, mean = 0.14412212371826172\n",
      "\n",
      "\n",
      "Text = stripes\n",
      "Dog: min = 0.07493738830089569, max = 0.1875077486038208, mean = 0.13668864965438843\n",
      "Cat: min = 0.05654824152588844, max = 0.20238636434078217, mean = 0.13531909883022308\n",
      "\n",
      "\n",
      "Text = eyes\n",
      "Dog: min = 0.10503299534320831, max = 0.20599983632564545, mean = 0.1572762280702591\n",
      "Cat: min = 0.07684467732906342, max = 0.207831472158432, mean = 0.15472492575645447\n",
      "\n",
      "\n",
      "Text = nose\n",
      "Dog: min = 0.10976484417915344, max = 0.2223389446735382, mean = 0.17259672284126282\n",
      "Cat: min = 0.10403107106685638, max = 0.21943992376327515, mean = 0.1683548390865326\n",
      "\n",
      "\n",
      "Text = ear\n",
      "Dog: min = 0.074224092066288, max = 0.20752651989459991, mean = 0.1535474956035614\n",
      "Cat: min = 0.0744754746556282, max = 0.21176330745220184, mean = 0.15245066583156586\n",
      "\n",
      "\n",
      "Text = mouth\n",
      "Dog: min = 0.10456445813179016, max = 0.22483012080192566, mean = 0.16418518126010895\n",
      "Cat: min = 0.09552758187055588, max = 0.22498981654644012, mean = 0.16394168138504028\n",
      "\n",
      "\n",
      "Text = face\n",
      "Dog: min = 0.1063414067029953, max = 0.21906085312366486, mean = 0.1638144999742508\n",
      "Cat: min = 0.08888137340545654, max = 0.2313423752784729, mean = 0.16246318817138672\n",
      "\n",
      "\n",
      "Text = claw\n",
      "Dog: min = 0.09348255395889282, max = 0.20047011971473694, mean = 0.157491534948349\n",
      "Cat: min = 0.08141801506280899, max = 0.19861292839050293, mean = 0.15832562744617462\n",
      "\n",
      "\n",
      "Text = teeth\n",
      "Dog: min = 0.09659615904092789, max = 0.2101026177406311, mean = 0.15340319275856018\n",
      "Cat: min = 0.09539894014596939, max = 0.21686668694019318, mean = 0.15348051488399506\n",
      "\n",
      "\n",
      "Text = wiskers\n",
      "Dog: min = 0.1025434359908104, max = 0.20467661321163177, mean = 0.16543103754520416\n",
      "Cat: min = 0.08388841897249222, max = 0.2223542034626007, mean = 0.1680469661951065\n",
      "\n",
      "\n",
      "Text = cheekbone\n",
      "Dog: min = 0.04796070605516434, max = 0.18299056589603424, mean = 0.14370042085647583\n",
      "Cat: min = 0.047143470495939255, max = 0.18573912978172302, mean = 0.14115551114082336\n",
      "\n",
      "\n",
      "Text = chest\n",
      "Dog: min = 0.10100400447845459, max = 0.19543705880641937, mean = 0.1565864235162735\n",
      "Cat: min = 0.08578889816999435, max = 0.20195482671260834, mean = 0.15637007355690002\n",
      "\n",
      "\n",
      "Text = thigh\n",
      "Dog: min = 0.08177401125431061, max = 0.1898052841424942, mean = 0.1523597240447998\n",
      "Cat: min = 0.07966386526823044, max = 0.19267237186431885, mean = 0.15150906145572662\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"bark\", \"meow\" - sound features not image features\n",
    "texts_list = [\"bark\", \"meow\", \"tail\", \"fur\", \"stripes\", \"eyes\", \"nose\", \"ear\", \"mouth\", \"face\", \"claw\", \"teeth\", \"wiskers\", \"cheekbone\", \"chest\", \"thigh\"]\n",
    "\n",
    "for texts in texts_list:\n",
    "\n",
    "    dog_sim = getSimilarity(texts, dog_tf)\n",
    "    cat_sim = getSimilarity(texts, cat_tf)\n",
    "\n",
    "    print(f\"Text = {texts}\")\n",
    "    print(f\"Dog: min = {dog_sim.min()}, max = {dog_sim.max()}, mean = {dog_sim.mean()}\")\n",
    "    print(f\"Cat: min = {cat_sim.min()}, max = {cat_sim.max()}, mean = {cat_sim.mean()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"wheels\"]\n",
    "\n",
    "text_tokens = clip.tokenize(texts).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max similarity score = 0.2002001851797104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_features /= train_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ train_features.cpu().numpy().T\n",
    "\n",
    "print(f\"Max similarity score = {similarity.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 50000), 0.06968014, 0.20020019, 0.14867409)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.shape, similarity.min(), similarity.max(), similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "\n",
    "def load(url):\n",
    "    \"\"\"\n",
    "    Given an url of an image, downloads the image and\n",
    "    returns a PIL image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    pil_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    # convert to BGR format\n",
    "    image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "    return image\n",
    "\n",
    "\n",
    "def imshow(img, caption, ShowFig = True, SaveFig = True, path = 'tmp.png'):\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.figtext(0.5, 0.09, caption, wrap=True, horizontalalignment='center', fontsize=20)\n",
    "\n",
    "    if SaveFig:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if ShowFig:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# def loadLocalImage(image_path):\n",
    "#     pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "#     # convert to BGR format\n",
    "#     image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "#     return image\n",
    "\n",
    "\n",
    "def loadLocalImage(image_path):\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "#     # convert to BGR format\n",
    "#     image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "    return pil_image\n",
    "\n",
    "    \n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "\n",
    "def saveAsGIF(image_list, video_name='temp.gif', fps=30):\n",
    "    imageio.mimsave(video_name, image_list, fps = fps)\n",
    "\n",
    "\n",
    "def saveAsVideo(image_list, video_name='temp.avi', fps=30):\n",
    "    # out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), fps = 15, frameSize = len(image_list))\n",
    "    out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), fps = fps, frameSize = image_list[0].shape[:2])\n",
    "    # out = cv2.VideoWriter(filename = video_name, apiPreference = cv2.CAP_FFMPEG,  fourcc = cv2.VideoWriter_fourcc(*'DIVX'), fps = fps, frameSize = image_list[0].shape[:2])\n",
    "    for img in image_list:\n",
    "        out.write(img)\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(original_images, texts, similarity):\n",
    "    \n",
    "    # count = len(descriptions)\n",
    "    # count = len(texts)\n",
    "    # count = len(original_images)\n",
    "    count = max(len(texts), len(original_images))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "    # plt.colorbar()\n",
    "    # plt.yticks(range(count), texts, fontsize=18)\n",
    "    plt.yticks(range(len(texts)), texts, fontsize=18)\n",
    "    plt.xticks([])\n",
    "    for i, image in enumerate(original_images):\n",
    "        plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "    for x in range(similarity.shape[1]):\n",
    "        for y in range(similarity.shape[0]):\n",
    "            plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "      plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "    plt.xlim([-0.5, count - 0.5])\n",
    "    plt.ylim([count + 0.5, -2])\n",
    "\n",
    "    plt.title(\"Cosine similarity between text and image features\", size=20)\n",
    "    \n",
    "\n",
    "def plotTopKImages(original_images, similarity, top_k = 5):\n",
    "\n",
    "    # best_img_idx = similarity.argmax()\n",
    "    # print(best_img_idx)\n",
    "    # original_images[best_img_idx]\n",
    "    \n",
    "    similarity = similarity.squeeze(0)\n",
    "    top_k_idx = np.argpartition(similarity, -top_k)[-top_k:]\n",
    "\n",
    "    top_k_similarity = similarity[top_k_idx]\n",
    "#     top_k_images = original_images[top_k_idx]\n",
    "\n",
    "    sort_idx = top_k_similarity.argsort()\n",
    "    sort_idx = sort_idx[::-1] #Reverse to get from max to min\n",
    "    top_k_idx = top_k_idx[sort_idx]\n",
    "    \n",
    "    top_k_similarity = top_k_similarity[sort_idx]\n",
    "    print(f\"top_k_similarity = {top_k_similarity}\")\n",
    "    \n",
    "#     top_k_images = top_k_images[sort_idx]\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 5*int(len(top_k_idx)/3)+1))\n",
    "\n",
    "    for idx, top_idx in enumerate(top_k_idx):\n",
    "        plt.subplot(int(len(top_k_idx)/3)+1, 3, idx + 1)\n",
    "        plt.imshow(original_images[top_idx])\n",
    "        plt.title(f\"Similarity score = {similarity[top_idx]}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plotTopKTexts(original_images, similarity, top_k = 5):\n",
    "\n",
    "    # best_img_idx = similarity.argmax()\n",
    "    # print(best_img_idx)\n",
    "    # original_images[best_img_idx]\n",
    "    \n",
    "    similarity = similarity.squeeze(0)\n",
    "    top_k_idx = np.argpartition(similarity, -top_k)[-top_k:]\n",
    "\n",
    "    top_k_similarity = similarity[top_k_idx]\n",
    "#     top_k_images = original_images[top_k_idx]\n",
    "\n",
    "    sort_idx = top_k_similarity.argsort()\n",
    "    sort_idx = sort_idx[::-1] #Reverse to get from max to min\n",
    "    top_k_idx = top_k_idx[sort_idx]\n",
    "    \n",
    "    top_k_similarity = top_k_similarity[sort_idx]\n",
    "    print(f\"top_k_similarity = {top_k_similarity}\")\n",
    "    \n",
    "#     top_k_images = top_k_images[sort_idx]\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 5*int(len(top_k_idx)/3)+1))\n",
    "\n",
    "    for idx, top_idx in enumerate(top_k_idx):\n",
    "        plt.subplot(int(len(top_k_idx)/3)+1, 3, idx + 1)\n",
    "        plt.imshow(original_images[top_idx])\n",
    "        plt.title(f\"Similarity score = {similarity[top_idx]}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageResize(image, size):\n",
    "#     return cv2.resize(i, size, interpolation = cv2.INTER_CUBIC)\n",
    "    return cv2.resize(i, size)\n",
    "\n",
    "def addCrop(image, h, w, crop_orientation = 'height', no_crops = 5):\n",
    "    \n",
    "    if crop_orientation == 'height':\n",
    "        crops = [image[i*int(h/no_crops):(i+1)*int(h/no_crops), :, :] for i in range(no_crops)]\n",
    "        crops = [cv2.resize(i, (w,h)) for i in crops] #Resize image\n",
    "#         crops = [imageResize(i, (w,h)) for i in crops] #Resize image\n",
    "        crops = [Image.fromarray(i) for i in crops] #Convert to PIL Image\n",
    "    elif crop_orientation == 'width':\n",
    "        crops = [image[:, i*int(w/no_crops):(i+1)*int(w/no_crops), :] for i in range(no_crops)]\n",
    "        crops = [cv2.resize(i, (w,h)) for i in crops] #Resize image\n",
    "#         crops = [imageResize(i, (w,h)) for i in crops] #Resize image\n",
    "        crops = [Image.fromarray(i) for i in crops] #Convert to PIL Image\n",
    "    elif crop_orientation == 'both':\n",
    "#         crops = [[image[i*int(h/no_crops):(i+1)*int(h/no_crops), j*int(w/no_crops):(j+1)*int(w/no_crops), :] for i in range(no_crops)] for j in range(no_crops)]\n",
    "        \n",
    "        crops = []\n",
    "        [[crops.append(image[i*int(h/no_crops):(i+1)*int(h/no_crops), j*int(w/no_crops):(j+1)*int(w/no_crops), :]) for i in range(no_crops)] for j in range(no_crops)]        \n",
    "    \n",
    "        crops = [cv2.resize(i, (w,h)) for i in crops] #Resize image\n",
    "#         crops = [imageResize(i, (w,h)) for i in crops] #Resize image\n",
    "        crops = [Image.fromarray(i) for i in crops] #Convert to PIL Image\n",
    "    else:\n",
    "        raise Exception(f\"Error! Undefined crop_orientation = {crop_orientation}\")\n",
    "        \n",
    "    return crops\n",
    "\n",
    "def generateCandidateCrops(org_image):\n",
    "\n",
    "    all_images = [org_image]\n",
    "\n",
    "    image = np.array(org_image)[:, :, [2, 1, 0]]\n",
    "    print(image.shape)\n",
    "\n",
    "    h,w,c = image.shape\n",
    "\n",
    "    all_images += addCrop(image, h, w, crop_orientation = 'height', no_crops = 3)\n",
    "    all_images += addCrop(image, h, w, crop_orientation = 'width', no_crops = 3)\n",
    "    all_images += addCrop(image, h, w, crop_orientation = 'both', no_crops = 3)\n",
    "    \n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'height', no_crops = 5)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'width', no_crops = 5)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'both', no_crops = 5)\n",
    "    \n",
    "    \n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'height', no_crops = 10)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'width', no_crops = 10)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'both', no_crops = 10)\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCLIP(org_image, captions, useCanditdateCrops = True):\n",
    "\n",
    "#     original_images = [org_image]\n",
    "    if useCanditdateCrops:\n",
    "        original_images = generateCandidateCrops(org_image)\n",
    "    elif isinstance(org_image, list):\n",
    "        original_images = org_image\n",
    "    else:\n",
    "        original_images = [org_image]\n",
    "        \n",
    "#     image = preprocess(org_image)\n",
    "    image_inputs = [preprocess(i) for i in original_images]\n",
    "\n",
    "#     image_inputs = [image for i in range(10)]\n",
    "    image_inputs = torch.tensor(np.stack(image_inputs)).cuda()\n",
    "\n",
    "    if isinstance(captions, str):\n",
    "        texts = [captions]\n",
    "    else:\n",
    "        texts = captions\n",
    "    # text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "    text_tokens = clip.tokenize(texts).cuda()\n",
    "    \n",
    "    \n",
    "    # image_input = torch.tensor(np.stack(image)).cuda()\n",
    "    # text_token = clip.tokenize(caption).cuda()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # image_features = model.encode_image(image_inputs).float()\n",
    "        image_features = model.encode_image(image_inputs)\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "    \n",
    "    print(image_features[0].shape, image_features[1].shape)\n",
    "    print(text_features.shape)\n",
    "    # print(image_features[1].reshape(-1, 4096).shape)\n",
    "    # image_features = torch.concat((image_features[0], image_features[1].reshape(-1, 1024)), dim = 0)\n",
    "    image_features = torch.concat((image_features[0], image_features[1]), dim = 0)\n",
    "    print(image_features.shape)\n",
    "    \n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "#     print(similarity)\n",
    "    print(f\"Max similarity score = {similarity.max()}\")\n",
    "\n",
    "    return original_images, texts, similarity"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Interacting with CLIP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "739e91aeaec4e2241cc8be3aa97a61dccf4ec0ff3dd190c10338099ca0ffe193"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12e23e2819094ee0a079d4eb77cfc4f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1369964d45004b5e95a058910b2a33e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a5f52e56ede4ac3abe37a3ece007dc9",
       "IPY_MODEL_ce8b0faa1a1340b5a504d7b3546b3ccb"
      ],
      "layout": "IPY_MODEL_12e23e2819094ee0a079d4eb77cfc4f9"
     }
    },
    "161969cae25a49f38aacd1568d3cac6c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a61c10fc00c4f04bb00b82e942da210": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e6adc4592124a4581b85f4c1f3bab4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7a5f52e56ede4ac3abe37a3ece007dc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a61c10fc00c4f04bb00b82e942da210",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e6adc4592124a4581b85f4c1f3bab4d",
      "value": 169001437
     }
    },
    "b597cd6f6cd443aba4bf4491ac7f957e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce8b0faa1a1340b5a504d7b3546b3ccb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_161969cae25a49f38aacd1568d3cac6c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b597cd6f6cd443aba4bf4491ac7f957e",
      "value": " 169001984/? [00:06&lt;00:00, 25734958.25it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
