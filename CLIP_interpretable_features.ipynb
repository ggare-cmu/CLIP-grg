{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPHN7PJgKOzb"
   },
   "source": [
    "# Interacting with CLIP\n",
    "\n",
    "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1hkDT38hSaP",
    "outputId": "70a44964-883d-4fd0-b95a-2c7f2b19aca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLFS29hnhlY4",
    "outputId": "11779e1e-8bdd-4167-c18e-d26bdd6b67db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBRVTY9lbGm8",
    "outputId": "f06fd2fd-6126-475b-87d0-b10aa3b7da49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 244M/244M [00:41<00:00, 6.15MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 102,007,137\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "# model, preprocess = clip.load(\"ViT-B/32\")\n",
    "# model, preprocess = clip.load(\"ViT-L/14\")\n",
    "# model, preprocess = clip.load(\"RN50x64\")\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21slhZGCqANb"
   },
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
    "\n",
    "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6cpiIFHp9N6",
    "outputId": "880cb98e-1e5e-430e-8b59-4bf35fa554f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f28d7650ee0>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Replace the original image pre-processor as we don't want to normalize the image using other dataset stats\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        ToTensor(),\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        # CenterCrop(n_px),\n",
    "        # _convert_image_to_rgb,\n",
    "        # ToTensor(),\n",
    "        # Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) #For CIFAR-10 dataset\n",
    "    ])\n",
    "\n",
    "\n",
    "# preprocess = _transform(model.input_resolution.item())\n",
    "preprocess = _transform(model.visual.input_resolution)\n",
    "\n",
    "print(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='~/data', train=True, download=False,\n",
    "                            transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                        #   batch_size=args.batch_size,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = datasets.CIFAR10(root='~/data', train=False, download=False,\n",
    "                           transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trainset.data\n",
    "train_label = trainset.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/grg/Research/CLIP-grg/CLIP_interpretable_features.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/grg/Research/CLIP-grg/CLIP_interpretable_features.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m preprocess(train_data[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/anaconda3/envs/glip/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/glip/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/glip/lib/python3.8/site-packages/torchvision/transforms/transforms.py:349\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    342\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/glip/lib/python3.8/site-packages/torchvision/transforms/functional.py:436\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    434\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    435\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, max_size\u001b[39m=\u001b[39;49mmax_size)\n\u001b[1;32m    438\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39msize, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, max_size\u001b[39m=\u001b[39mmax_size, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/glip/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py:233\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39munused\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresize\u001b[39m(\n\u001b[1;32m    226\u001b[0m     img: Image\u001b[39m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     max_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[1;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pil_image(img):\n\u001b[0;32m--> 233\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, Sequence) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))):\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "preprocess(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     image = preprocess(org_image)\n",
    "image_inputs = [preprocess(i) for i in original_images]\n",
    "\n",
    "#     image_inputs = [image for i in range(10)]\n",
    "image_inputs = torch.tensor(np.stack(image_inputs)).cuda()\n",
    "\n",
    "\n",
    "# text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "text_tokens = clip.tokenize(texts).cuda()\n",
    "\n",
    "\n",
    "# image_input = torch.tensor(np.stack(image)).cuda()\n",
    "# text_token = clip.tokenize(caption).cuda()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # image_features = model.encode_image(image_inputs).float()\n",
    "    image_features = model.encode_image(image_inputs)\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "\n",
    "print(image_features[0].shape, image_features[1].shape)\n",
    "print(text_features.shape)\n",
    "# print(image_features[1].reshape(-1, 4096).shape)\n",
    "# image_features = torch.concat((image_features[0], image_features[1].reshape(-1, 1024)), dim = 0)\n",
    "image_features = torch.concat((image_features[0], image_features[1]), dim = 0)\n",
    "print(image_features.shape)\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "#     print(similarity)\n",
    "print(f\"Max similarity score = {similarity.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "\n",
    "def load(url):\n",
    "    \"\"\"\n",
    "    Given an url of an image, downloads the image and\n",
    "    returns a PIL image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    pil_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    # convert to BGR format\n",
    "    image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "    return image\n",
    "\n",
    "\n",
    "def imshow(img, caption, ShowFig = True, SaveFig = True, path = 'tmp.png'):\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.figtext(0.5, 0.09, caption, wrap=True, horizontalalignment='center', fontsize=20)\n",
    "\n",
    "    if SaveFig:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if ShowFig:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# def loadLocalImage(image_path):\n",
    "#     pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "#     # convert to BGR format\n",
    "#     image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "#     return image\n",
    "\n",
    "\n",
    "def loadLocalImage(image_path):\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "#     # convert to BGR format\n",
    "#     image = np.array(pil_image)[:, :, [2, 1, 0]]\n",
    "    return pil_image\n",
    "\n",
    "    \n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "\n",
    "def saveAsGIF(image_list, video_name='temp.gif', fps=30):\n",
    "    imageio.mimsave(video_name, image_list, fps = fps)\n",
    "\n",
    "\n",
    "def saveAsVideo(image_list, video_name='temp.avi', fps=30):\n",
    "    # out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), fps = 15, frameSize = len(image_list))\n",
    "    out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), fps = fps, frameSize = image_list[0].shape[:2])\n",
    "    # out = cv2.VideoWriter(filename = video_name, apiPreference = cv2.CAP_FFMPEG,  fourcc = cv2.VideoWriter_fourcc(*'DIVX'), fps = fps, frameSize = image_list[0].shape[:2])\n",
    "    for img in image_list:\n",
    "        out.write(img)\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(original_images, texts, similarity):\n",
    "    \n",
    "    # count = len(descriptions)\n",
    "    # count = len(texts)\n",
    "    # count = len(original_images)\n",
    "    count = max(len(texts), len(original_images))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "    # plt.colorbar()\n",
    "    # plt.yticks(range(count), texts, fontsize=18)\n",
    "    plt.yticks(range(len(texts)), texts, fontsize=18)\n",
    "    plt.xticks([])\n",
    "    for i, image in enumerate(original_images):\n",
    "        plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "    for x in range(similarity.shape[1]):\n",
    "        for y in range(similarity.shape[0]):\n",
    "            plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "      plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "    plt.xlim([-0.5, count - 0.5])\n",
    "    plt.ylim([count + 0.5, -2])\n",
    "\n",
    "    plt.title(\"Cosine similarity between text and image features\", size=20)\n",
    "    \n",
    "\n",
    "def plotTopKImages(original_images, similarity, top_k = 5):\n",
    "\n",
    "    # best_img_idx = similarity.argmax()\n",
    "    # print(best_img_idx)\n",
    "    # original_images[best_img_idx]\n",
    "    \n",
    "    similarity = similarity.squeeze(0)\n",
    "    top_k_idx = np.argpartition(similarity, -top_k)[-top_k:]\n",
    "\n",
    "    top_k_similarity = similarity[top_k_idx]\n",
    "#     top_k_images = original_images[top_k_idx]\n",
    "\n",
    "    sort_idx = top_k_similarity.argsort()\n",
    "    sort_idx = sort_idx[::-1] #Reverse to get from max to min\n",
    "    top_k_idx = top_k_idx[sort_idx]\n",
    "    \n",
    "    top_k_similarity = top_k_similarity[sort_idx]\n",
    "    print(f\"top_k_similarity = {top_k_similarity}\")\n",
    "    \n",
    "#     top_k_images = top_k_images[sort_idx]\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 5*int(len(top_k_idx)/3)+1))\n",
    "\n",
    "    for idx, top_idx in enumerate(top_k_idx):\n",
    "        plt.subplot(int(len(top_k_idx)/3)+1, 3, idx + 1)\n",
    "        plt.imshow(original_images[top_idx])\n",
    "        plt.title(f\"Similarity score = {similarity[top_idx]}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plotTopKTexts(original_images, similarity, top_k = 5):\n",
    "\n",
    "    # best_img_idx = similarity.argmax()\n",
    "    # print(best_img_idx)\n",
    "    # original_images[best_img_idx]\n",
    "    \n",
    "    similarity = similarity.squeeze(0)\n",
    "    top_k_idx = np.argpartition(similarity, -top_k)[-top_k:]\n",
    "\n",
    "    top_k_similarity = similarity[top_k_idx]\n",
    "#     top_k_images = original_images[top_k_idx]\n",
    "\n",
    "    sort_idx = top_k_similarity.argsort()\n",
    "    sort_idx = sort_idx[::-1] #Reverse to get from max to min\n",
    "    top_k_idx = top_k_idx[sort_idx]\n",
    "    \n",
    "    top_k_similarity = top_k_similarity[sort_idx]\n",
    "    print(f\"top_k_similarity = {top_k_similarity}\")\n",
    "    \n",
    "#     top_k_images = top_k_images[sort_idx]\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 5*int(len(top_k_idx)/3)+1))\n",
    "\n",
    "    for idx, top_idx in enumerate(top_k_idx):\n",
    "        plt.subplot(int(len(top_k_idx)/3)+1, 3, idx + 1)\n",
    "        plt.imshow(original_images[top_idx])\n",
    "        plt.title(f\"Similarity score = {similarity[top_idx]}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageResize(image, size):\n",
    "#     return cv2.resize(i, size, interpolation = cv2.INTER_CUBIC)\n",
    "    return cv2.resize(i, size)\n",
    "\n",
    "def addCrop(image, h, w, crop_orientation = 'height', no_crops = 5):\n",
    "    \n",
    "    if crop_orientation == 'height':\n",
    "        crops = [image[i*int(h/no_crops):(i+1)*int(h/no_crops), :, :] for i in range(no_crops)]\n",
    "        crops = [cv2.resize(i, (w,h)) for i in crops] #Resize image\n",
    "#         crops = [imageResize(i, (w,h)) for i in crops] #Resize image\n",
    "        crops = [Image.fromarray(i) for i in crops] #Convert to PIL Image\n",
    "    elif crop_orientation == 'width':\n",
    "        crops = [image[:, i*int(w/no_crops):(i+1)*int(w/no_crops), :] for i in range(no_crops)]\n",
    "        crops = [cv2.resize(i, (w,h)) for i in crops] #Resize image\n",
    "#         crops = [imageResize(i, (w,h)) for i in crops] #Resize image\n",
    "        crops = [Image.fromarray(i) for i in crops] #Convert to PIL Image\n",
    "    elif crop_orientation == 'both':\n",
    "#         crops = [[image[i*int(h/no_crops):(i+1)*int(h/no_crops), j*int(w/no_crops):(j+1)*int(w/no_crops), :] for i in range(no_crops)] for j in range(no_crops)]\n",
    "        \n",
    "        crops = []\n",
    "        [[crops.append(image[i*int(h/no_crops):(i+1)*int(h/no_crops), j*int(w/no_crops):(j+1)*int(w/no_crops), :]) for i in range(no_crops)] for j in range(no_crops)]        \n",
    "    \n",
    "        crops = [cv2.resize(i, (w,h)) for i in crops] #Resize image\n",
    "#         crops = [imageResize(i, (w,h)) for i in crops] #Resize image\n",
    "        crops = [Image.fromarray(i) for i in crops] #Convert to PIL Image\n",
    "    else:\n",
    "        raise Exception(f\"Error! Undefined crop_orientation = {crop_orientation}\")\n",
    "        \n",
    "    return crops\n",
    "\n",
    "def generateCandidateCrops(org_image):\n",
    "\n",
    "    all_images = [org_image]\n",
    "\n",
    "    image = np.array(org_image)[:, :, [2, 1, 0]]\n",
    "    print(image.shape)\n",
    "\n",
    "    h,w,c = image.shape\n",
    "\n",
    "    all_images += addCrop(image, h, w, crop_orientation = 'height', no_crops = 3)\n",
    "    all_images += addCrop(image, h, w, crop_orientation = 'width', no_crops = 3)\n",
    "    all_images += addCrop(image, h, w, crop_orientation = 'both', no_crops = 3)\n",
    "    \n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'height', no_crops = 5)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'width', no_crops = 5)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'both', no_crops = 5)\n",
    "    \n",
    "    \n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'height', no_crops = 10)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'width', no_crops = 10)\n",
    "    # all_images += addCrop(image, h, w, crop_orientation = 'both', no_crops = 10)\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCLIP(org_image, captions, useCanditdateCrops = True):\n",
    "\n",
    "#     original_images = [org_image]\n",
    "    if useCanditdateCrops:\n",
    "        original_images = generateCandidateCrops(org_image)\n",
    "    elif isinstance(org_image, list):\n",
    "        original_images = org_image\n",
    "    else:\n",
    "        original_images = [org_image]\n",
    "        \n",
    "#     image = preprocess(org_image)\n",
    "    image_inputs = [preprocess(i) for i in original_images]\n",
    "\n",
    "#     image_inputs = [image for i in range(10)]\n",
    "    image_inputs = torch.tensor(np.stack(image_inputs)).cuda()\n",
    "\n",
    "    if isinstance(captions, str):\n",
    "        texts = [captions]\n",
    "    else:\n",
    "        texts = captions\n",
    "    # text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "    text_tokens = clip.tokenize(texts).cuda()\n",
    "    \n",
    "    \n",
    "    # image_input = torch.tensor(np.stack(image)).cuda()\n",
    "    # text_token = clip.tokenize(caption).cuda()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # image_features = model.encode_image(image_inputs).float()\n",
    "        image_features = model.encode_image(image_inputs)\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "    \n",
    "    print(image_features[0].shape, image_features[1].shape)\n",
    "    print(text_features.shape)\n",
    "    # print(image_features[1].reshape(-1, 4096).shape)\n",
    "    # image_features = torch.concat((image_features[0], image_features[1].reshape(-1, 1024)), dim = 0)\n",
    "    image_features = torch.concat((image_features[0], image_features[1]), dim = 0)\n",
    "    print(image_features.shape)\n",
    "    \n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "#     print(similarity)\n",
    "    print(f\"Max similarity score = {similarity.max()}\")\n",
    "\n",
    "    return original_images, texts, similarity"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Interacting with CLIP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "739e91aeaec4e2241cc8be3aa97a61dccf4ec0ff3dd190c10338099ca0ffe193"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12e23e2819094ee0a079d4eb77cfc4f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1369964d45004b5e95a058910b2a33e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a5f52e56ede4ac3abe37a3ece007dc9",
       "IPY_MODEL_ce8b0faa1a1340b5a504d7b3546b3ccb"
      ],
      "layout": "IPY_MODEL_12e23e2819094ee0a079d4eb77cfc4f9"
     }
    },
    "161969cae25a49f38aacd1568d3cac6c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a61c10fc00c4f04bb00b82e942da210": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e6adc4592124a4581b85f4c1f3bab4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7a5f52e56ede4ac3abe37a3ece007dc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a61c10fc00c4f04bb00b82e942da210",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e6adc4592124a4581b85f4c1f3bab4d",
      "value": 169001437
     }
    },
    "b597cd6f6cd443aba4bf4491ac7f957e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce8b0faa1a1340b5a504d7b3546b3ccb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_161969cae25a49f38aacd1568d3cac6c",
      "placeholder": "​",
      "style": "IPY_MODEL_b597cd6f6cd443aba4bf4491ac7f957e",
      "value": " 169001984/? [00:06&lt;00:00, 25734958.25it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
